# Bogdan Sikorsky

## Data Engineer

#### Python | API | ETL | SQL | Scraping | Automation

### Kyiv, Ukraine

**[WebSite](https://bit.ly/Website_Bogdan_Sikorsky) |
[LinkedIn](https://bit.ly/LinkedIn_Bogdan_sikorsky) |
[UpWork](https://bit.ly/UpWork_Bogdan_Sikorsky) |
[GitHub](https://bit.ly/GitHub_Bogdan_Sikorsky) |
[CV](https://bit.ly/CV_Bogdan_Sikorsky_) |
[YouTube](https://bit.ly/YouTube_Bogdan_Sikorsky) |
[Email](mailto:bogdan.sikorsky.dev@gmail.com)**

# Career

In recent years I worked full-time for a US company which provides real estate market analysis. My position was a Data Engineer. I worked on improving huge and complicated ETL processes.

Overall, I have worked as a Python developer focused on data solutions since 2020. I started with small freelance projects from private clients. My first projects were automation tasks such as calculating, cleaning and transforming data with Python scripts.

Later on, I learned how to scrape data from the web and started working on multiple moderate-difficulty projects. The goal was to create scrapers for e-commerce, real estate, etc websites, store collected data and create APIs for further data distribution.

# Experience

## Data Engineer

### Nancy Packes Inc

_March 2022 – January 2024_

I was responsible for developing and maintaining hundreds of scrapers for single-page websites and large platforms with protections and JavaScript. I was supporting an automated weekly scraping process and created different logs and metrics.

I developed solutions for data cleaning because we had a lot of polluted data from all the different sources. Overall, we had more than 100k-1m records during each scraping.

#### Achievements

- I adopted an asynchronous scraping technique that significantly reduced single scraping session time which was causing troubles. My teammates were applying my module for themselves.
- I created a filtering pipeline that separated polluted data which we were not able to filter or clean manually.
- I created a couple of metrics and reports that helped us deeply understand the quality of our data.
- I created a comprehensive logging system that was far beyond what my teammates had.

**Tech Stack**: ```Requests, Selenium, AsyncIO, AioHTTP, LXML, BS4, Pandas, Pydantic, SQL, Jenkins, Azure DevOps, Google APIs.```

## Python Developer

### Freelance

_October 2020 – March 2022_

I had different projects from UpWork, LinkedIn and even my YouTube. Usually, they lasted from a week to several months so there is no particular project worth stand-alone mentioning.

Projects were often quite similar and required creating scrapers, cleaning data, saving data to the database, automation collection data on regular basics, and APIs creation to distribute data.

#### Achievements

- I gained enough experience to compete for better roles and projects and finally become a full-time Data Engineer.

**Tech Stack**: ```FastAPI, MongoDB, Pandas, Pydantic, Docker, Google Cloud Platform, Scrapy, Selenium, Git.```

# Projects

### WebRiderAsync

_Asynchronous Web Requests Utility_

I developed a Python-based asynchronous utility designed for efficient handling of large volumes of web requests using `aiohttp` and `asyncio`. WebRiderAsync simplifies the process of making parallel HTTP requests, has simple settings and is suitable for both API interaction and web scraping. Published on [PyPI](https://pypi.org/project/webrider-async/) with ongoing updates and with sources accessible on [GitHub](https://github.com/bogdan-sikorsky/webrider_async).

# Skills

> **Technical**: Python, FastAPI, Streamlit, AsyncIO, Aiohttp, Scrapy, Splash, Requests, Selenium, Pandas, Numpy, LXML, Pydantic, Beautiful Soup, Flask, Django

> **Databases**: SQL, MongoDB

> **DevOps**: Git, CI/CD, Docker, Docker-Compose, Makefile, Traefik, Celery, RabbitMQ, Jenkins, Uvicorn, Shell Script, Linux, Windows

> **Cloud Services**: Google Cloud Platform (GCP), Amazon Web Services (AWS), Azure DevOps, Cloudflare, GoDaddy

> **Monitoring Tools**: Sentry, NetData, UptimeRobot, Google Analytics, LogRocket Analytics, PostHog Analytics

> **Services**: ChatGPT API, Google Cloud Storage, Google Translate API, Google Text To Speech API, Google Sheets API, Nominatim API

# Education

- Strategic communications at Taras Shevchenko National University of Kyiv (Master, 2023-2025)
- Communications at Taras Shevchenko National University of Kyiv (Bachelor, 2013-2018)
- International relationships at Kyiv Slavonic University (2009-2012)

# Certificates

- [Python (Basic) Certificate by Hackerrank.com](https://www.hackerrank.com/certificates/d32ad5a6f887)
- [SQL (Basic) Certificate by Hackerrank.com](https://www.hackerrank.com/certificates/c383d100da8d)
- [SQL (Intermediate) Certificate by Hackerrank.com](https://www.hackerrank.com/certificates/1e77a3c646f5)

# Goals

I am passionate about diving into new interesting projects to work with data. Also, interested in full-stack dev a bit and AI technologies. I’m having fun learning new Python instruments. Wish to grow and deliver interesting projects at the same time!

# Languages

English, Ukrainian
